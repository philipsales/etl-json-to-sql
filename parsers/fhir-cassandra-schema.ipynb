{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from os import path\n",
    "from os import walk\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import logging\n",
    "import pandas_profiling\n",
    "from pandas.io.json import json_normalize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['address',\n",
       " 'birthDate',\n",
       " 'communication',\n",
       " 'deceasedDateTime',\n",
       " 'gender',\n",
       " 'id',\n",
       " 'identifier',\n",
       " 'maritalStatus.coding',\n",
       " 'maritalStatus.text',\n",
       " 'multipleBirthBoolean',\n",
       " 'name',\n",
       " 'resourceType',\n",
       " 'telecom',\n",
       " 'text.div',\n",
       " 'text.status']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../data/fhir-json/2rows/Patient/1.Patient.json\") as f:\n",
    "    d = json.load(f)\n",
    "\n",
    "awhcuris = json_normalize(d)\n",
    "address_data = awhcuris.head(31)\n",
    "address_data.T\n",
    "\n",
    "data_list = list(address_data)\n",
    "data_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flattenDict(d, result=None):\n",
    "    \n",
    "    if result is None:\n",
    "        result = {}\n",
    "        \n",
    "    for key, value in list(d.items()):\n",
    "        value = d[key]\n",
    "        \n",
    "        #TODO: OPtimiza strip clean\n",
    "        if isinstance(value, str):\n",
    "            value = value.replace(\"'\", ' ')\n",
    "                \n",
    "        if isinstance(value, dict):\n",
    "            #print('THIS IS DICT: ========')\n",
    "            #print(value)\n",
    "            value1 = {}\n",
    "            for keyIn in value:\n",
    "                #print('DICT keyIn :', keyIn)\n",
    "                value1[\".\".join([key,keyIn])]=value[keyIn.lower()]\n",
    "            flattenDict(value1, result)\n",
    "        elif isinstance(value, (list, tuple)):  \n",
    "            #print('THIS IS TUPEL: ', value)\n",
    "            for indexB, element in enumerate(value):\n",
    "                if isinstance(element, dict):\n",
    "                    value1 = {}\n",
    "                    index = 0\n",
    "                    for keyIn in element:\n",
    "                        #print('TUPLES keyIn :', keyIn)\n",
    "                        #print('key :', key)\n",
    "                        newkey = \".\".join([key,keyIn])        \n",
    "                        value1[\".\".join([key,keyIn])]=value[indexB][keyIn]\n",
    "                        index += 1\n",
    "                    for keyA in value1:\n",
    "                        flattenDict(value1, result)   \n",
    "        else: \n",
    "            result[key.lower()]=value\n",
    "            \n",
    "            #columns.append(key.lower())\n",
    "            #print('[key]: ', key)\n",
    "            \n",
    "            \n",
    "            #print('result[key]: ', result[key])\n",
    "            \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeDirectory:\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        self.path = path\n",
    "\n",
    "    def create_dir(self, folders=None):\n",
    "        dirName = ''\n",
    "        \n",
    "        if isinstance(folders, list):\n",
    "            \n",
    "            for folder in folders:\n",
    "                dirName =  self.path + '/' + folder\n",
    "                try:\n",
    "                    os.makedirs(dirName)\n",
    "                    print(\"Directory \" , dirName ,  \" Created \") \n",
    "                except FileExistsError:\n",
    "                    print(\"Directory \" , dirName ,  \" already exists\")\n",
    "        else:\n",
    "            os.makedirs(dirName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Folder directory Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FileIterator:\n",
    "    \n",
    "    def __init__(self, input_root, input_folders):\n",
    "        self.input_data_dir = input_root\n",
    "        self.input_folders = input_folders\n",
    "        \n",
    "    def iterate_filenames(self):\n",
    "        resources = self.__iterate_dirfiles()\n",
    "        \n",
    "        files = self.__iterate_file(resources)\n",
    "        \n",
    "        return files\n",
    "        \n",
    "    def __iterate_dirfiles(self):\n",
    "        path_files = []\n",
    "        file_names = self.input_folders\n",
    "\n",
    "        for file in file_names:\n",
    "            path_files.append(input_data_dir + '/' + file)\n",
    "        return path_files\n",
    "    \n",
    "    def __iterate_file(self, path_dbs):\n",
    "        path_db_workload_files = []     \n",
    "        for path_db in path_dbs:\n",
    "            for root, dirs, files in walk(path_db):  \n",
    "                for filename in files:\n",
    "                    if not filename.endswith(('.json~', '.swp', '-checkpoint.json')):\n",
    "                        print('valid file: ',filename)\n",
    "                        path_db_workload_files.append(root + '/' + filename)\n",
    "                    if ''.join(dirs) != '.ipynb_checkpoints':\n",
    "                        pass\n",
    "\n",
    "        return path_db_workload_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filecontent reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentReader():\n",
    "    \n",
    "    def __init__(self, filename_paths):\n",
    "        self.filename_paths = filename_paths\n",
    "        \n",
    "    def get_data(self):\n",
    "        data = []\n",
    "        for filename_path in self.filename_paths:\n",
    "            \n",
    "            json = ReadJSON()\n",
    "            data.append(json.read(filename_path))\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadJSON():\n",
    "    \n",
    "    @staticmethod\n",
    "    def read(file_path):\n",
    "        with open(file_path) as f:\n",
    "            d = json.load(f)\n",
    "        return flattenDict(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Flatten FHIR JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeResult():\n",
    "    def __init__(self, data, source_paths, destination_path):\n",
    "        self.source_paths = source_paths\n",
    "        self.data = data\n",
    "        self.destination_path = destination_path\n",
    "        \n",
    "    def outputs(self):      \n",
    "        for resource in self.data:\n",
    "           \n",
    "            destination_path = self.destination_path + '/' + resource['resourcetype']\n",
    "            #print('destination: ' ,destination_path)\n",
    "            id = resource['id'].split('-')[0]\n",
    "     \n",
    "            f = open(destination_path + '/' + id + '.' + resource['resourcetype'] + '.json', 'w')\n",
    "            f.write(json.dumps(resource))\n",
    "            f.close()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ../output/fhir-json-cassandra/100/Patient  already exists\n",
      "valid file:  0.Patient.json\n",
      "valid file:  1.Patient.json\n",
      "valid file:  10.Patient.json\n",
      "valid file:  11.Patient.json\n",
      "valid file:  12.Patient.json\n",
      "valid file:  13.Patient.json\n",
      "valid file:  14.Patient.json\n",
      "valid file:  15.Patient.json\n",
      "valid file:  16.Patient.json\n",
      "valid file:  17.Patient.json\n",
      "valid file:  18.Patient.json\n",
      "valid file:  19.Patient.json\n",
      "valid file:  2.Patient.json\n",
      "valid file:  20.Patient.json\n",
      "valid file:  21.Patient.json\n",
      "valid file:  22.Patient.json\n",
      "valid file:  23.Patient.json\n",
      "valid file:  24.Patient.json\n",
      "valid file:  25.Patient.json\n",
      "valid file:  26.Patient.json\n",
      "valid file:  27.Patient.json\n",
      "valid file:  28.Patient.json\n",
      "valid file:  29.Patient.json\n",
      "valid file:  3.Patient.json\n",
      "valid file:  30.Patient.json\n",
      "valid file:  31.Patient.json\n",
      "valid file:  32.Patient.json\n",
      "valid file:  33.Patient.json\n",
      "valid file:  34.Patient.json\n",
      "valid file:  35.Patient.json\n",
      "valid file:  36.Patient.json\n",
      "valid file:  37.Patient.json\n",
      "valid file:  38.Patient.json\n",
      "valid file:  39.Patient.json\n",
      "valid file:  4.Patient.json\n",
      "valid file:  40.Patient.json\n",
      "valid file:  41.Patient.json\n",
      "valid file:  42.Patient.json\n",
      "valid file:  43.Patient.json\n",
      "valid file:  44.Patient.json\n",
      "valid file:  45.Patient.json\n",
      "valid file:  46.Patient.json\n",
      "valid file:  47.Patient.json\n",
      "valid file:  48.Patient.json\n",
      "valid file:  49.Patient.json\n",
      "valid file:  5.Patient.json\n",
      "valid file:  50.Patient.json\n",
      "valid file:  51.Patient.json\n",
      "valid file:  52.Patient.json\n",
      "valid file:  53.Patient.json\n",
      "valid file:  54.Patient.json\n",
      "valid file:  55.Patient.json\n",
      "valid file:  56.Patient.json\n",
      "valid file:  57.Patient.json\n",
      "valid file:  58.Patient.json\n",
      "valid file:  59.Patient.json\n",
      "valid file:  6.Patient.json\n",
      "valid file:  60.Patient.json\n",
      "valid file:  61.Patient.json\n",
      "valid file:  62.Patient.json\n",
      "valid file:  63.Patient.json\n",
      "valid file:  64.Patient.json\n",
      "valid file:  65.Patient.json\n",
      "valid file:  66.Patient.json\n",
      "valid file:  67.Patient.json\n",
      "valid file:  68.Patient.json\n",
      "valid file:  69.Patient.json\n",
      "valid file:  7.Patient.json\n",
      "valid file:  70.Patient.json\n",
      "valid file:  71.Patient.json\n",
      "valid file:  72.Patient.json\n",
      "valid file:  73.Patient.json\n",
      "valid file:  74.Patient.json\n",
      "valid file:  75.Patient.json\n",
      "valid file:  76.Patient.json\n",
      "valid file:  77.Patient.json\n",
      "valid file:  78.Patient.json\n",
      "valid file:  79.Patient.json\n",
      "valid file:  8.Patient.json\n",
      "valid file:  80.Patient.json\n",
      "valid file:  81.Patient.json\n",
      "valid file:  82.Patient.json\n",
      "valid file:  83.Patient.json\n",
      "valid file:  84.Patient.json\n",
      "valid file:  85.Patient.json\n",
      "valid file:  86.Patient.json\n",
      "valid file:  87.Patient.json\n",
      "valid file:  88.Patient.json\n",
      "valid file:  89.Patient.json\n",
      "valid file:  9.Patient.json\n",
      "valid file:  90.Patient.json\n",
      "valid file:  91.Patient.json\n",
      "valid file:  92.Patient.json\n",
      "valid file:  93.Patient.json\n",
      "valid file:  94.Patient.json\n",
      "valid file:  95.Patient.json\n",
      "valid file:  96.Patient.json\n",
      "valid file:  97.Patient.json\n",
      "valid file:  98.Patient.json\n",
      "valid file:  99.Patient.json\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n",
      "result is NONE\n"
     ]
    }
   ],
   "source": [
    "root_dir = '../'\n",
    "\n",
    "input_data_folders = ['Patient']\n",
    "input_data_dir = root_dir + 'output/fhir-json/100'\n",
    "\n",
    "output_directory = root_dir + 'output/fhir-json-cassandra/100'\n",
    "\n",
    "directory = MakeDirectory(output_directory)\n",
    "directory.create_dir(input_data_folders)\n",
    "    \n",
    "files = FileIterator(input_data_dir, input_data_folders)\n",
    "input_dirpaths = files.iterate_filenames()\n",
    "input_dirpaths\n",
    "\n",
    "data = []\n",
    "files = ContentReader(input_dirpaths)\n",
    "data = files.get_data()\n",
    "\n",
    "result = MakeResult(data, input_dirpaths, output_directory)\n",
    "result.outputs()\n",
    "\n",
    "#TODO OPTIMIZE array \n",
    "#TODO OPTIMIZE DYNAMIC FIELDS DDL GENERATOR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
